{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Definición de la clase UNet (la versión adaptada del artículo base de referencia)\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        \n        self.conv1 = self._double_conv(1, 64) # Primer bloque de convolución: convierte de 1 canal (imagen en escala de grises) a 64 canales\n        self.pool1 = nn.MaxPool2d(2)    # MaxPooling: reduce el tamaño espacial a la mitad\n        \n        self.conv2 = self._double_conv(64, 128) # Segundo bloque de convolución: convierte de 64 a 128 canales\n        self.drop1 = nn.Dropout()   # Dropout para regularización (reduce el overfitting)\n        self.pool2 = nn.MaxPool2d(2) # MaxPooling: reduce el tamaño espacial a la mitad nuevamente\n        \n        self.conv3 = self._double_conv(128, 256)  # Tercer bloque de convolución (parte más profunda de la red): convierte de 128 a 256 canales\n        \n        self.upconv1 = nn.Sequential(  # Primera capa de deconvolución para la expansión (Decoder)\n          nn.Dropout(), # Dropout para regularización\n          nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),  # Deconvolución para aumentar el tamaño espacial\n          nn.ReLU(inplace=True)  # Activación ReLU\n        )\n        self.conv4 = self._double_conv(256, 128)   # Bloque de convolución para procesar los datos combinados tras la concatenación\n        \n        self.upconv2 = nn.Sequential(  # Segunda capa de deconvolución\n          nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), # Deconvolución\n          nn.ReLU(inplace=True) # Activación ReLU\n        )        \n        self.conv5 = self._double_conv(128, 64)  # Bloque de convolución para procesar los datos combinados tras la segunda concatenación\n\n        self.output = nn.Conv2d(64, 1, kernel_size=1)  # Capa final de convolución: reduce a un solo canal de salida\n        \n    def _double_conv(self, in_channels, out_channels):   #Función que nos permite realizar dos convoluciones seguidas con activaciones ReLU.\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x):   # Función que representa el flujo de la arquitectura\n        conv1 = self.conv1(x)  \n        pool1 = self.pool1(conv1)\n        \n        conv2 = self.conv2(pool1)\n        drop1 = self.drop1(conv2)\n        pool2 = self.pool2(drop1)\n        \n        conv3 = self.conv3(pool2)\n        \n        upconv1 = self.upconv1(conv3)\n        cat1 = torch.cat([upconv1, conv2], dim=1) # Primera concatenación con características del Encoder\n        conv4 = self.conv4(cat1)\n        \n        upconv2 = self.upconv2(conv4)\n        cat2 = torch.cat([upconv2, conv1], dim=1) # Segunda concatenación con características del Encoder\n        conv5 = self.conv5(cat2)\n        \n        output = self.output(conv5)\n        \n        return torch.sigmoid(output)  # Salida: pasa por una convolución y luego se aplica una función sigmoide\n      \nmodel_unet = UNet()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:30:57.382696Z","iopub.execute_input":"2024-12-09T14:30:57.383234Z","iopub.status.idle":"2024-12-09T14:30:57.427357Z","shell.execute_reply.started":"2024-12-09T14:30:57.383191Z","shell.execute_reply":"2024-12-09T14:30:57.425756Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Definición de la clase SegNet (la versión adaptada del artículo base de referencia)\nclass SegNet(nn.Module):\n    def __init__(self, num_classes):\n        super(SegNet, self).__init__()\n\n        self.conv1_1 = nn.Conv2d(1, 64, 3, padding=1) # Primer bloque de convolución: convierte de 1 canal (imagen en escala de grises) a 64 canales\n        self.conv1_2 = nn.Conv2d(64, 128, 3, padding=1) # Convierte de 64 a 128 canales\n        self.max_pooling1 = nn.MaxPool2d(2, stride=2, return_indices=True) # MaxPooling con índices para \"unpooling\"\n        self.conv2_1 = nn.Conv2d(128, 256, 3, padding=1) # Segundo bloque de convolución: de 128 a 256 canales\n        self.conv2_2 = nn.Conv2d(256, 512, 3, padding=1) # de 256 a 512 canales\n        self.max_pooling2 = nn.MaxPool2d(2, stride=2, return_indices=True) # Segundo MaxPooling con índices\n        self.conv3_1 = nn.Conv2d(512, 512, 3, padding=1)  # Tercer bloque de convolución:mantiene 512 canales\n        self.conv3_2 = nn.Conv2d(512, 512, 3, padding=1)  \n        self.max_pooling3 = nn.MaxPool2d(2, stride=2, return_indices=True) # Tercer MaxPooling con índices\n\n        # Decoder\n        self.max_unpooling1 = nn.MaxUnpool2d(2, stride=2)  # Primer \"unpooling\" para revertir el último MaxPooling\n        self.conv4_1 = nn.Conv2d(512, 512, 3, padding=1)  # Convolución posterior al \"unpooling\"\n        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1) # Segunda convolución del bloque\n        self.max_unpooling2 = nn.MaxUnpool2d(2, stride=2) # Segundo \"unpooling\"\n        self.conv5_1 = nn.Conv2d(512, 256, 3, padding=1) # Convolución posterior, de 512 a 256 canales\n        self.conv5_2 = nn.Conv2d(256, 128, 3, padding=1) # Segunda convolución del bloque, de 256 a 128 canales\n        self.max_unpooling3 = nn.MaxUnpool2d(2, stride=2) # Tercer \"unpooling\"\n        self.conv6_1 = nn.Conv2d(128, 64, 3, padding=1) # Convolución posterior, de 128 a 64 canales\n        self.conv6_2 = nn.Conv2d(64, num_classes, 3, padding=1) # Última convolución, de 64 a num_classes\n\n        # Capas necesarias en ambas etapas\n        self.relu = nn.ReLU()  # Activación ReLU\n        self.sigmd = nn.Sigmoid() #Activación sigmoide\n        self.batchn4 = nn.BatchNorm2d(512) # Normalización para canales de 512\n        self.batchn3 = nn.BatchNorm2d(256) # Normalización para canales de 256\n        self.batchn2 = nn.BatchNorm2d(128) # Normalización para canales de 128\n        self.batchn1 = nn.BatchNorm2d(64) # Normalización para canales de 64\n        self.batchn0 = nn.BatchNorm2d(1) # Normalización para canal de entrada\n\n    def forward(self, img):   # Flujo de la arquitectura\n        img = self.conv1_1(img)\n        img = self.batchn1(img)\n        img = self.relu(img)\n        img = self.conv1_2(img)\n        img = self.batchn2(img)\n        img = self.relu(img)\n        img, ind1 = self.max_pooling1(img)\n\n        img = self.conv2_1(img)\n        img = self.batchn3(img)\n        img = self.relu(img)\n        img = self.conv2_2(img)\n        img = self.batchn4(img)\n        img = self.relu(img)\n        img, ind2 = self.max_pooling2(img)\n\n        img = self.conv3_1(img)\n        img = self.batchn4(img)\n        img = self.relu(img)\n        img = self.conv3_2(img)\n        img = self.batchn4(img)\n        img = self.relu(img)\n        img, ind3 = self.max_pooling3(img)\n\n        img = self.max_unpooling1(img, ind3)\n        img = self.conv4_1(img)\n        img = self.batchn4(img)\n        img = self.relu(img)\n        img = self.conv4_2(img)\n        img = self.batchn4(img)\n        img = self.relu(img)\n\n        img = self.max_unpooling2(img, ind2)\n        img = self.conv5_1(img)\n        img = self.batchn3(img)\n        img = self.relu(img)\n        img = self.conv5_2(img)\n        img = self.batchn2(img)\n        img = self.relu(img)\n\n        img = self.max_unpooling3(img, ind1)\n        img = self.conv6_1(img)\n        img = self.batchn1(img)\n        img = self.relu(img)\n        img = self.conv6_2(img)\n        img = self.sigmd(img)\n\n        return img\n\nnum_classes = 1   # Número de clases en la salida\nmodel_segnet = SegNet(num_classes)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Definición de la clase UNetCompleto (la versión original de Ronneberger et al.) \nclass UNetCompleto(nn.Module):    #Es igual que el UNet de antes pero con más capas convolucionales\n    def __init__(self, num_classes=1):\n        super(UNetCompleto, self).__init__()\n\n        # Ruta Contractiva\n        self.conv1 = self._double_conv(1, 64) \n        self.pool1 = nn.MaxPool2d(2)           \n        \n        self.conv2 = self._double_conv(64, 128)\n        self.pool2 = nn.MaxPool2d(2)           \n        \n        self.conv3 = self._double_conv(128, 256)\n        self.pool3 = nn.MaxPool2d(2)          \n        \n        self.conv4 = self._double_conv(256, 512)\n        self.pool4 = nn.MaxPool2d(2)           \n        \n        self.bottleneck = self._double_conv(512, 1024)\n        \n        # Ruta Expansiva\n        self.up6 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2) \n        self.conv6 = self._double_conv(1024, 512)                        \n        \n        self.up7 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)   \n        self.conv7 = self._double_conv(512, 256)                         \n        \n        self.up8 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) \n        self.conv8 = self._double_conv(256, 128)                          \n        \n        self.up9 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.conv9 = self._double_conv(128, 64)                         \n        \n        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)      \n    def _double_conv(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x):\n        conv1 = self.conv1(x)\n        pool1 = self.pool1(conv1)\n        \n        conv2 = self.conv2(pool1)\n        pool2 = self.pool2(conv2)\n        \n        conv3 = self.conv3(pool2)\n        pool3 = self.pool3(conv3)\n        \n        conv4 = self.conv4(pool3)\n        pool4 = self.pool4(conv4)\n        \n        bottleneck = self.bottleneck(pool4)\n\n        up6 = self.up6(bottleneck)\n        up6 = torch.cat([up6, conv4], dim=1) \n        conv6 = self.conv6(up6)\n        \n        up7 = self.up7(conv6)\n        up7 = torch.cat([up7, conv3], dim=1)\n        conv7 = self.conv7(up7)\n        \n        up8 = self.up8(conv7)\n        up8 = torch.cat([up8, conv2], dim=1)\n        conv8 = self.conv8(up8)\n        \n        up9 = self.up9(conv8)\n        up9 = torch.cat([up9, conv1], dim=1)\n        conv9 = self.conv9(up9)\n        \n        # Capa final\n        output = self.final_conv(conv9)\n        return torch.sigmoid(output)\n\nmodel_unet_completo = UNetCompleto(num_classes=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\n\nsummary(model_unet, input_size=(8, 1, 256, 256)) # 1,861,697 params entrenables\nsummary(model_segnet, input_size=(8, 1, 256, 256)) # 12,540,291 parametros entrenables","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Definimos las rutas de los directorios de imágenes y máscaras\nimages_dir = 'data/covid19/im/256'\nmasks_dir = 'data/covid19/mask/binary/256'\n# images_dir = 'data/retina/images'\n# masks_dir = 'data/retina/masks'\n\nfrom torchvision.transforms import v2 # Importamos el módulo de transformaciones para preprocesamiento\n\n\n# Cconjunto de transformaciones para las imágenes y máscaras\ntransform = v2.Compose([\n    v2.Resize((256, 256)),\n    v2.RandomVerticalFlip(p=0.5),            \n    v2.RandomHorizontalFlip(p=0.5),          \n    v2.RandomAffine(degrees=0, shear=[-10, 10, -10, 10]),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True)\n])\n\ndataset = SegmentationDataset(images_dir, masks_dir, transform=transform)  # Creamos un dataset de segmentación con las imágenes, máscaras y transformaciones definidas\ndata_loader = DataLoader(dataset, batch_size=8, shuffle=True)  # Cargamos el dataset completo en un DataLoader para iterar sobre los datos en lotes (batches) de manera eficiente\n\n# Dividimos el dataset en conjuntos de entrenamiento, validación y prueba\ntrain_size = int(0.72 * len(dataset)) # 0.72\nval_size = int(0.1 * len(dataset)) # 0.10\ntest_size = len(dataset) - train_size - val_size # 0.18\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size]) # Realizamos la partición utilizando random_split\n\nbatch_size = 8  # Definimos el tamaño de lote (batch size)\n\n# Creamos DataLoaders para cada conjunto\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Imprimimos información sobre la distribución de los datos\nprint(f\"Dataset (total): {len(dataset)} samples\")\nprint(f\"Train dataset: {len(train_dataset)} samples\")\nprint(f\"Validation dataset: {len(val_dataset)} samples\")\nprint(f\"Test dataset: {len(test_dataset)} samples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\nfor i, (image, mask) in enumerate(random.sample(list(dataset), 2)):\n  axs[i, 0].imshow(image.squeeze(0), cmap='gray')\n  axs[i, 0].set_title(f'Imagen')\n  axs[i, 0].axis('off')\n  \n  axs[i, 1].imshow(mask.squeeze(0), cmap='gray')\n  axs[i, 1].set_title(f'Máscara')\n  axs[i, 1].axis('off')\n  \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def entrenar(model, device, train_loader, val_loader, epochs=10, lr=0.001, weight_decay=0.0001, step_size=12, gamma=0.95):  #Función para entrenar un modelo de segmentación utilizando PyTorch.\n   # Parámetros: modelo a entrenar, dispositivo a utilizar, DataLoader para los datos de entrenamiento, DataLoader para los datos de validación, nº de épocas de entrenamiento, tasa de aprendizaje inicial, factor de penalización para la regularización L2, nº de épocas después de las cuales se reduce la tasa de aprendizaje, factor multiplicativo para ajustar la tasa de aprendizaje\n    model = model.to(device) # Enviamos el modelo al dispositivo (GPU o CPU)\n    print(\"training on: \", device)\n    criterion = nn.BCELoss()   # Definimos la función de pérdida basada en el error cuadrático para problemas de segmentación binaria\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) # Optimización con Adam\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma) # Scheduler para ajustar dinámicamente la tasa de aprendizaje\n    \n    val_losses, train_losses = [], []  # Inicializamos listas para almacenar las pérdidas de entrenamiento y validación\n    \n    for epoch in range(epochs):  # Por cada época\n        model.train() # Ponemos el modelo en modo de entrenamiento\n        train_loss = 0.0  # Inicializamos la pérdida de entrenamiento en 0\n        for images, masks in train_loader:  # Iteramos sobre los lotes de datos de entrenamiento\n            images, masks = images.to(device), masks.to(device)  # Enviamos las imágenes y máscaras al dispositivo\n            \n            outputs = model(images)     # Obtenemos las predicciones del modelo          \n            loss = criterion(outputs, masks)  # Calculamos la pérdida entre las predicciones y las máscaras verdaderas\n\n             # Realizamos la retropropagación y actualización de pesos\n            optimizer.zero_grad() # Limpiamos los gradientes acumulados\n            loss.backward() # Calculamos los gradientes\n            optimizer.step() # Actualizamos los pesos\n            \n            train_loss += loss.item()   # Acumulamos la pérdida de entrenamiento\n\n        # Evaluación del modelo en el conjunto de validación\n        model.eval() # Ponemos el modelo en modo de evaluación \n        val_loss = 0.0  # Inicializamos la pérdida de validación en 0\n        for images, masks in val_loader:\n            images, masks = images.to(device), masks.to(device) # Enviamos los datos al dispositivo\n            outputs = model(images)    # Obtenemos las predicciones del modelo\n            loss = criterion(outputs, masks)   # Calculamos la pérdida de validación\n            val_loss += loss.item()\n\n         # Calculamos las pérdidas promedio de entrenamiento y validación\n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n\n         # Almacenamos las pérdidas promedio\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n\n        \n        scheduler.step()   # Actualizamos la tasa de aprendizaje según el scheduler\n    return (train_losses, val_losses) # Devolvemos las pérdidas para análisis posterior","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# determinar el dispositivo\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# entrenar el modelo adaptivo de U-Net\nu1_t_losses, u1_v_losses = entrenar(model_unet, device, train_loader, val_loader, epochs=10)\ntorch.save(model_unet.state_dict(), \"model_unet.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# entrenar el modelo adaptivo de SegNet\ns1_t_losses, s1_v_losses = entrenar(model_segnet, device, train_loader, val_loader, epochs=10)\ntorch.save(model_segnet.state_dict(), \"model_segnet.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# entrenar el modelo original de U-Net\nu2_t_losses, u2_v_losses = entrenar(model_unet_completo, device, train_loader, val_loader, epochs=50)\ntorch.save(model_unet_completo.state_dict(), \"model_unet_completo.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_train_val_losses(u1_t_losses, u1_v_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(u1_t_losses, label='Pérdida de entrenamiento')\n    plt.plot(u1_v_losses, label='Pérdida de validación')\n    plt.xlabel('Época')\n    plt.ylabel('Pérdida')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_train_val_losses(u1_t_losses, u1_v_losses)\nplot_train_val_losses(s1_t_losses, s1_v_losses)\nplot_train_val_losses(u2_t_losses, u2_v_losses)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_precisions(model, device, dataloader, model_name):\n    model.eval()\n    total, correct = 0, 0\n    intersection, union = 0, 0\n\n    with torch.no_grad():\n        for images, masks in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n\n            outputs = model(images)\n            predictions = (outputs > 0.5).float()\n\n            # Calcular la precisión de píxeles\n            correct += (predictions == masks).sum().item()\n            total += masks.numel()\n            \n            # Calcular la IoU\n            pred_mask = (predictions == 0)\n            true_mask = (masks == 0)\n\n            intersection += (pred_mask & true_mask).sum().item()\n            union += (pred_mask | true_mask).sum().item()\n            \n    accuracy = correct / total\n    iou = (intersection / union) if union > 0 else 0\n    print(f\"Modelo: {model_name} - Precisión de píxeles: {accuracy*100:.2f}% - IoU: {iou*100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_precisions(model_unet, device, test_loader, 'U-Net')  \nprint_precisions(model_segnet, device, test_loader, 'SegNet')\nprint_precisions(model_unet_completo, device, test_loader, 'U-Net Completo')  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ndef display_predictions(model, device, dataloader): \n  model.eval()\n  \n  with torch.no_grad():\n    fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n    \n    for images, masks in dataloader:\n      images, masks = images.to(device), masks.to(device)\n      outputs = model(images)\n      outputs = (outputs > 0.5).float()\n      \n      for i in range(3):\n        image = images[i].squeeze().cpu()\n        mask = masks[i].squeeze().cpu()\n        pred_mask = outputs[i].squeeze().cpu()\n        \n        axs[i, 0].imshow(image, cmap='gray')\n        axs[i, 0].set_title(f'Imagen')\n        axs[i, 0].axis('off')\n        \n        axs[i, 1].imshow(mask, cmap='gray')\n        axs[i, 1].set_title(f'Máscara')\n        axs[i, 1].axis('off')\n        \n        axs[i, 2].imshow(pred_mask, cmap='gray')\n        axs[i, 2].set_title(f'Predicción')\n        axs[i, 2].axis('off')\n        \n      break\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_predictions(model_unet, device, test_loader)\ndisplay_predictions(model_segnet, device, test_loader)\ndisplay_predictions(model_unet_completo, device, test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}